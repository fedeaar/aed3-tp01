\vspace{0.5em}
El problema de la selección de actividades que consideraremos tiene la siguiente premisa. Dado un conjunto 
\begin{equation}\\\nonumber
    A := \{a_1\ ...\ a_n \}
\end{equation}
de actividades disponibles, donde, para todo $1 \leq i \leq n$, la actividad $a_i$ se representa por el intervalo $[s_i,\ t_i)$ ---que corresponde, respectivamente, al tiempo de inicio y finalización de la actividad---,  queremos encontrar un subconjunto de $A$ que nos permita realizar la mayor cantidad de actividades posibles. 

En esto, queremos elegir las actividades con la restricción que, para todo par de actividades $a_i$ y $a_j$ seleccionadas, $1 \leq i,\ j \leq n$, $a_i$ y $a_j$ no se solapen en el tiempo. Es decir, $s_i \geq t_j$ o $s_j \geq t_i$. Por ejemplo, si
\begin{equation}\\\nonumber
    A := \{[1,\ 3),\ [1,\ 4),\ [3,\ 6),\ [4,\ 7),\ [7,\ 8)\}
\end{equation}
\noindent luego las soluciones óptimas son $\{[1,\ 3),\ [3,\ 6),\ [7,\ 8)\}$ y $\{[1,\ 4),\ [4,\ 7),\ [7,\ 8)\}$. 

Como condición extra, limitaremos los tiempos para las actividades de manera tal que $1 \leq s_i < t_i \leq 2n$. 

\vspace{0.5em}
\subsection{Demostración de la propiedad de subestructura óptima}

Como preámbulo a la aplicación de una estrategia \textit{golosa} para resolver el problema, veamos primero que el problema tiene la propiedad de \textit{subestructura óptima}. 

\begin{proof}[Demostración]
    Consideremos una solución óptima $S \subset A$ que contiene, sin pérdida de generalidad, a alguna actividad $a_i \in A$ para $1 \leq i \leq n$. Luego, podemos particionar tanto a $S$ como a $A$ entre aquellas actividades que terminan antes que comience $a_i$ y aquellas que comienzan después de que termine $a_i$. 

    Si estas particiones de $S$ no son, correspondientemente, óptimas para las particiones de $A$, entonces existe algún subconjunto de las actividades que terminan antes que comience $a_i$ que tiene un tamaño mayor que la partición de $S$ que consideramos y, de igual manera, existe una solución mayor para el subconjunto de actividades que comienzan después de que termine $a_i$.
    
    Sigue entonces que $S$ no es óptima, ya que podemos formar una solución mejor a partir de estos otros dos subconjuntos, lo que es un absurdo.
\end{proof}

%Luego, estámos en condiciones de evaluar el problema como una serie de decisiones secuenciales. La siguiente demostración muestra que, además, las decisiones pueden ser locales. 

\vspace{0.5em}
\subsection{Demostración de la propiedad de selección golosa} Consideremos ahora la siguiente estrategia \textit{golosa}: elegir la actividad que finalice primero, de entre todas las actividades que sigan disponibles. Vamos a demostrar que esta estrategia es correcta.

\begin{proof}[Demostración]
    Notemos que, si la solución parcial $B_1\ ...\ B_i$ que brinda el algoritmo goloso en el paso $i$, para todo $0 \leq i \leq n$, se puede extender a una solución óptima, entonces la estrategia es correcta. Lo demostraremos por inducción.

    Para el caso base, $i = 0$, el algoritmo todavía no eligió ninguna actividad. Luego, podemos extender la solución a una solución optima de manera trivial.

    Supongamos ahora, para $i > 0$, que tenemos una solución parcial $B_1\ ...\ B_i$ de actividades elegidas por nuestro algoritmo que se puede extender a una solución óptima
    \begin{align}\nonumber
        B_1\ ...\ B_i,\ C_{i+1}\ ...\ C_j
    \end{align}
    donde, sin pérdida de generalidad, vamos a suponer que la secuencia de actividades sin solapamiento $C_{i+1}\ ...\ C_j$ está ordenada por tiempo de finalización. 

    Notar que, por nuestro método de selección, $B_1\ ...\ B_i$ también debe estar ordenado de la misma manera. Luego, todas las actividades $C_{i+1}\ ...\ C_j$ deben empezar después de que termine $B_i$ para que la extensión sea válida. Si no, habría solapamientos, ya que, por definición de la estrategia, deben terminar después que $B_i$.

    Consideremos ahora la solución parcial golosa $B_1\ ...\ B_{i+1}$, donde $B_{i+1}$ es la actividad cuyo momento final ocurre antes entre todas las actividades restantes y no se solapa con las actividades ya seleccionadas. 

    Como $B_{i+1}$ debe terminar antes que $C_{i+1}$ o $B_{i+1} = C_{i+1}$, entonces $B_{i+1}$ no puede solaparse con ninguna actividad $C_{i+2}\ ...\ C_j$. Esto se debe a que $C_{i+1}$ no lo hacía y, por hipótesis inductiva, es la actividad que termina antes en la extensión. Luego, $B_1\ ...\ B_{i+1}$ se puede extender por reemplazo directo a la solución óptima 
    \begin{align}\nonumber
        B_1\ ...\ B_{i+1},\ C_{i+2}\ ...\ C_j
    \end{align}
\end{proof}

%\vspace{0.5em}
\subsection{El algoritmo} En base a estas propiedades, podemos considerar el siguiente algoritmo. 

\vspace{0.5em}
\lstinputlisting[mathescape=true, language=pseudo, label=actividades, caption={Pseudocódigo para la \textit{Selección de actividades}.}]{files/src/.code/act.pseudo}

El mismo itera sobre las actividades de $A$ en orden de finalización. En base a la estrategia golosa, decide incluir, o no, una actividad si y sólo si comienza después de que termine la última actividad que ya incluyó, cuyo tiempo de finalización se guarda en la variable $i$. Dado que no tiene sentido considerar tiempos negativos, inicializamos $i$ en $0$.

\vspace{0.5em}
\subsection{Complejidad temporal y espacial} El comportamiento de \textit{act} depende del método de ordenamiento que elijamos y de las características de la estructura subyacente al conjunto $P$ (en particular, si la inserción de un elemento se puede realizar en tiempo constante). 

Respecto al método de ordenamiento, dado que los tiempos de finalización estan acotados por $2n$, podemos utilizar \textit{counting sort} para garantizar una complejidad de peor caso, tanto temporal como espacial, de orden lineal. %Notar que \textit{counting sort} tiene complejidad $O(m + k)$ donde $m$ es la cantidad de elementos a ordenar y $k$ es el rango entre el valor mínimo y máximo en el conjunto. Luego, nuestro ordenamiento tiene complejidad $O(n + 2n) = O(n)$.

Respecto a las características de $P$, si se implementa como un arreglo de tamaño $n$ ---dado que una solución óptima tiene a lo sumo tamaño $n$---, sigue que el costo de inserción será constante. Sin embargo, deberémos tener cuidado de liberar el espacio excedente antes de retornar.

De estas observaciones, y el hecho de que, como son $n$ actividades, el costo temporal del ciclo es $O(n)$, sigue que tanto la complejidad temporal como espacial de peor caso de \textit{act} es $O(n)$.