\vspace{1em}
El problema de la selección de actividades que consideraremos tiene la siguiente premisa. Dado un conjunto 
\begin{equation}\\\nonumber
    A := \{a_1\ ...\ a_n \}
\end{equation}
de actividades disponibles, donde, para todo $1 \leq i \leq n$, la actividad $a_i$ se representa por el intervalo $[s_i,\ t_i)$ ---que corresponde, respectivamente, al tiempo de inicio y finalización de la actividad---,  queremos encontrar un subconjunto de $A$ que nos permita realizar la mayor cantidad de actividades posibles. 

En esto, queremos elegir las actividades con la restricción que, para todo par de actividades $a_i$ y $a_j$ seleccionadas, $1 \leq i,\ j \leq n$, $a_i$ y $a_j$ no se solapen en el tiempo. Es decir, $s_i \geq t_j$ o $s_j \geq t_i$. Por ejemplo, si
\begin{equation}\\\nonumber
    A := \{[1,\ 3),\ [1,\ 4),\ [3,\ 6),\ [4,\ 7),\ [7,\ 8)\}
\end{equation}
\noindent luego algunas soluciones óptimas son $\{[1,\ 3),\ [3,\ 6),\ [7,\ 8)\}$ y $\{[1,\ 4),\ [4,\ 7),\ [7,\ 8)\}$. 

Como condición extra, limitaremos el tiempo de la actividad $a_i$, para todo $1 \leq i \leq n$, de manera tal que $1 \leq s_i < t_i \leq 2n$. 

%\vspace{0.5em}
\subsection{Demostración de la propiedad de subestructura óptima}

Como preámbulo a la aplicación de una estrategia \textit{golosa} para resolver el problema, veamos primero que el problema tiene la propiedad de \textit{subestructura óptima}. 

\begin{proof}[Demostración]
    Consideremos una solución óptima $S \subset A$ que contiene, sin pérdida de generalidad, a alguna actividad $a_i \in A$ para $1 \leq i \leq n$. Luego, podemos particionar tanto a $S$ como a $A$ entre aquellas actividades que terminan antes que comience $a_i$: $S_{<i}$ y $A_{<i}$; y aquellas que comienzan después de que termine $a_i$: $S_{>i}$ y $A_{>i}$. 

    Si estas particiones de $S$ no son, correspondientemente, óptimas para las particiones de $A$, entonces existe algún subconjunto de actividades $S'_{<i} \subset A_{<i}$ que tiene un tamaño mayor que la partición $S_{<i}$ que consideramos y, de igual manera, existe una solución mayor $S'_{>i}$ para el subconjunto de actividades $A_{>i}$.
    
    Sigue entonces que $S$ no es óptima, ya que podemos formar una solución mejor si consideramos $S'_{<i} \cup \{a_i\} \cup S'_{>i}$, lo que es un absurdo.
\end{proof}

Luego, estamos en condiciones de evaluar el problema dentro del marco teórico de la \textit{programación dinámica}\footnote{De seguir este camino, deberíamos demostrar, también, que el problema cuenta con la propiedad de \textit{solapamiento de subproblemas}.}. La siguiente demostración prueba que, además, podemos alcanzar un óptimo por medio de decisiones locales. 

%\vspace{0.5em}
\subsection{Demostración de la propiedad de selección golosa} Consideremos ahora la siguiente estrategia \textit{golosa}: elegir la actividad que finalice primero, de entre todas las actividades que sigan disponibles. Vamos a demostrar que esta estrategia es correcta.

\begin{proof}[Demostración]
    Notemos primero que, si una solución parcial $B_1\ ...\ B_i$ de actividades seleccionadas ---que brinde un algoritmo goloso que implemente esta estrategia--- se puede extender a una solución óptima, para todo $0 \leq i \leq n$, entonces la estrategia es correcta. Lo demostraremos por inducción.

    Para el caso base, $i = 0$, el algoritmo todavía no eligió ninguna actividad. Luego, podemos extender la solución a una solución óptima de manera trivial.

    Supongamos ahora, para $i > 0$, que tenemos una solución parcial $B_1\ ...\ B_i$ de actividades elegidas por nuestro algoritmo que se puede extender a una solución óptima
    \begin{align}\nonumber
        B_1\ ...\ B_i,\ C_{i+1}\ ...\ C_j
    \end{align}
    donde $j \leq n$ y, sin pérdida de generalidad, vamos a suponer que la secuencia de actividades sin solapamiento $C_{i+1}\ ...\ C_j$ está ordenada por tiempo de finalización. 

    Notar que, por nuestro método de selección, $B_1\ ...\ B_i$ también debe estar ordenado de la misma manera. Luego, todas las actividades $C_{i+1}\ ...\ C_j$ deben empezar después de que termine $B_i$ para que la extensión sea válida. Si no, habría solapamientos, ya que, por definición de la estrategia, deben terminar después que $B_i$.

    Consideremos ahora la solución parcial golosa $B_1\ ...\ B_{i+1}$, donde ---por el método de selección--- $B_{i+1}$ es la actividad cuyo momento final ocurre antes entre todas las actividades restantes y no se solapa con las actividades ya seleccionadas. 

    Como $B_{i+1}$ debe terminar antes que $C_{i+1}$ o $B_{i+1} = C_{i+1}$, entonces $B_{i+1}$ no puede solaparse con ninguna actividad $C_{i+2}\ ...\ C_j$. Esto se debe a que $C_{i+1}$ no lo hacía y, por hipótesis inductiva, es la actividad que termina antes en la extensión. Luego, $B_1\ ...\ B_{i+1}$ se puede extender por reemplazo directo a la solución óptima 
    \begin{align}\nonumber
        B_1\ ...\ B_{i+1},\ C_{i+2}\ ...\ C_j
    \end{align}
    lo que concluye la demostración.
\end{proof}

%\vspace{1em}
\subsection{El algoritmo} En base a estas propiedades, podemos considerar el siguiente algoritmo \textit{goloso} como una resolución al problema de la \textit{selección de actividades}. 

%\vspace{1em}
\lstinputlisting[mathescape=true, language=pseudo, label=actividades, caption={Pseudocódigo para la \textit{selección de actividades}.}]{files/src/.code/act.pseudo}

El mismo itera sobre las actividades en $A$ por orden de finalización. En base a la estrategia golosa, decide incluir, o no, una actividad si y sólo si comienza después de que termine la última actividad que ya se incluyó a la solución, cuyo tiempo de finalización se guarda en la variable $i$. Dado que no tiene sentido considerar tiempos negativos, inicializamos $i$ en $0$.

%\vspace{1em}
\subsection{Complejidad temporal y espacial} El comportamiento del algoritmo depende del método de ordenamiento que elijamos y de las características de la estructura subyacente al conjunto $P$ (en particular, si la inserción de un elemento se puede realizar en tiempo constante). 

Respecto al método de ordenamiento, dado que los tiempos de finalización estan acotados por $2n$, podemos utilizar \textit{counting sort} para garantizar una complejidad de peor caso, tanto temporal como espacial, de orden lineal. Notar que \textit{counting sort} tiene complejidad $\Theta(m + k)$ donde $m$ es la cantidad de elementos a ordenar y $k$ es la diferencia entre el valor máximo y mínimo en el conjunto. Para simplificar el algoritmo, vamos a trabajar con $k = 2n$ fijo. Luego, el ordenamiento es $\Theta(n + 2n) = \Theta(n)$.

Respecto a las características de $P$, si se implementa como un arreglo de tamaño $n$ ---dado que una solución óptima tiene a lo sumo esta cantidad de actividades---, sigue que el costo de inserción es constante. Sin embargo, debemos tener cuidado de liberar el espacio excedente luego de obtener un resultado.

De estas observaciones, y el hecho de que, como hay $n$ actividades, el costo temporal del ciclo es lineal, sigue que tanto la complejidad temporal como espacial de peor caso del algoritmo es $\Theta(n)$.